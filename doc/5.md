

关于群聊。

群聊是一个需要仔细考虑的设计，之前版本则是由首次发起消息者创建群聊任务，这样是没什么逻辑上的问题。

但是当时的设计则把消息推送一起交由群聊任务完成，这样在群聊很大，且消息很频繁时，会极大地降低性能(不用测试都可以猜到)。

这里以腾讯QQ最大2000人为例，如果每秒有100条消息，且所有人均在线，则一秒就有200,000次推送，这对于服务器来说完全不可接受，虽然这是极端场景，一般而言不存在这么多人同时在线且这么高速的发送频率。

所以我们(其实目前为止还是只有我一个人在写)决定把群聊推送分散到每台机器，即群聊任务仅向群成员所在的所有服务器推送，由具体服务器推送到具体用户，这样可以尽可能分散群聊任务所载机器的负载。

但是这样引入了额外的数据记录，即群聊用户 = 所在机器的映射关系，且需要在群聊成员变化(加入 退出)以及用户 = 机器映射关系变化时更新群聊任务记录的映射关系。

关于消息设计。

基本重构了，复用了尽可能多的数据段，缩减了8字节的消息头，且引入接收方所在节点记录。

此外就是消息先后顺序及持久化，我们需要一个高性能的分布式自增调用。持久化则选用定期向持久化节点推送的方式，或许是基于时间，或许是基于总量，再复杂一点可以二者均采用。

关于调度器。

调度器是整个服务端集群的核心，它掌控着很多关键服务，包括消息节点的伸缩。所以调度器一旦宕机，带来的后果不可接受。

所以调度器选用集群模式，且可以动态伸缩，但是最少为3个节点。

消息节点亦是动态伸缩，但是不同在于，调度器节点之间完全没有依赖，互相独立，调度器的依赖源于与之连接的节点产生的依赖，一旦集群稳定，所有消息节点均处于互联状态，则调度器部分不可用不会造成整体不可用。

消息节点彼此产生依赖，某一节点宕机会造成某些用户服务不可达。

此外服务端彼此通信要求Ack，但是基于异步消息流实现同步调用不如直接使用RPC，所以这里采用超时通知机制。

某条消息超时会触发指定次数重传，如果最后还是超时，则应该视为严重错误，因为这说明对端宕机或者对端负载到达了极限。

记录节点彼此没有依赖，互相独立，集群仅仅为了缓解每个节点的压力。

关于用户节点迁移：

- 调度器选择新的节点
- 发送迁移(包含新节点ID和用户ID)给旧节点和新节点(包含旧节点ID和用户ID)
- 旧节点发送迁移给客户端(如果连接存在)，如果不存在直接结束此步骤
- 每次旧节点收到消息，转发给新节点，直到收到新节点终止消息或者用户断开连接
- 新节点收到用户连接，发送终止消息给旧节点

所以其他节点可以放心根据消息的node_id属性转发消息而不必担心消息丢失，除非节点宕机，这属于致命的错误。

用户加入/退出群聊，涉及到群任务的成员遍历，所以必须保证消息节点响应群成员变化。

关于加入群聊：

- 用户申请加入群聊，推送加群消息到管理员节点
- 管理员用户得到提醒
- 通过/拒绝，并推送到申请者节点
- 申请者得到提醒
- 最后由接口端向调度器传达成员变化消息
- 调度器推送到所有节点

这里留意到，业务消息应该也被持久化和加入消息列表。

关于用户获取目标用户的node_id。我们需要一个迁移通知机制，即可以在目标用户所在的node发生迁移时，可以通知回发送方。一种简单的设计就是在原节点设置一个拦截，一旦收到指定receiver的消息便返回迁移消息。所以这里需要一种记录，去记录发生迁移的receiver，但是肯定不可以永久记录，不然内存占用会是一个持久问题，所以我们需要设置过期时间，但是同时又要保证不会有客户端因此丢失迁移信息，所以客户端也需要每隔一段时间重新拉去node_id映射。例如存活时间14天，则客户端需要在保持7天的间隔，这样可以保证在整个存活周期内，客户端至少更新了一次node_id映射。

关于群消息的推送。

如果某个成员连接断开，会导致quic重传，因而阻塞后面的成员消息推送，所以我们需要一个缓冲队列，而且这个队列长度应该是特异化的，不活跃用户则缓冲区小一些，活跃用户会有更大的缓冲区。以此保证quic断开时的重传阻塞不会影响后续连接的发送。

关于收件箱设计。

用户点击聊天框记录为最新查阅时间，之后直到断开连接时间-最大空闲时间为当前用户和peer的最新查阅时间。收件箱默认返回用户所有设备最晚下线时间-当前时间之间发送消息的peer列表。

不同客户端可以指定不同的最晚下线时间。

user-peer之间的新消息则由最新查阅时间-当前时间来确定，此法需要单独查询。

查阅时间跨客户端存在，收件箱依赖于具体客户端。